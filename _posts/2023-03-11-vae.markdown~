---
layout: post
title:  "Understand Diffusion Models with VAEs"
excerpt: "How DDPM is "
date:   2023-03-11 00:00:00 +0000
categories: []
hide: false
---


<!-- Mathjax Support -->
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
Some time ago, with the explosion of DALLE2 and stable diffusion, I also wanted to understand how these models tick, so I opened the [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239) (DDPM) paper. At the very beginning, I encountered this line:



![]({{ "/assets/vae/firstline.png" | absolute_url }})  
"the usual variational bound"...  

Ok, this “variational bound” is “usual” for these people. Clearly, I need to catch up with some of these concepts.
 In this post, Ill start from the very beginning - [Variational Auto Encoders](https://arxiv.org/abs/1312.6114) (VAE). I'll derive [ELBO](https://en.wikipedia.org/wiki/Evidence_lower_bound) and construct the VAE model and loss. Then, I will jump five years back to the DDPM paper, show why the objective is "usual", and discuss how it differs from the classic VAE.


# VAE - Problem Scenario
This is the VAE graphical model
![]({{ "/assets/vae/vae_graph.png" | absolute_url }}){:height="40%" width="40%"}  

$$X=\{x^{(i)}\}_{i=0}^N$$ is our observed dataset, For example, N i.i.d images of cats. These images came from an unknown distribution $$p_\theta(x)$$, which describes all the images of cats in the world.
$$z$$ is a latent variable that follows the distribution $$p_\theta(z)$$.
We assume that every cat image was generated with these steps:  
1. Draw $$z^{(i)}$$ from $$p_\theta(z)$$  
2. Generate $$x^{(i)}$$ from the conditional distribution $$p_\theta(x\vert z)$$

I want to visualize this to give some better intuition:  
![]({{ "/assets/vae/latentcat.png" | absolute_url }}){:height="100%" width="100%"}  
This process means that every cat image $$x^i$$ has a low dimensional representation $$z^i$$, and from $$z^i$$ we can generate the image by sampling $$p_\theta(x|z)$$. The fact that every cat has a low-dimensional representation shouldn't surprise us; we usually see the opposite process when we use neural nets. For example, when we train a convnet, the convnet will learn to represent an image $$x^i$$ in a lower dimensional space $$z^i$$ and then pass $$z^i$$ through a classification layer.  

Unfortunately, we do not know what the parameters $$\theta$$ in $$p_\theta(x\vert z)$$ and $$p_\theta(z)$$ are. If we knew, we could use them to generate infinite images of random cats using the steps from above.

To find the optimal $$\theta$$ to make $$p_\theta(x\vert z)$$ and $$p_\theta(z)$$ represent our dataset, we need to do what we always do in machine learning, try to estimate $$p_\theta(x)$$ with maximum likelihood. In other words, We need to find the parameters $$\theta$$ that maximize   

$$\theta^*=\underset{\theta}{\operatorname{arg max}} \prod^{i=N}_{i=0} p_\theta (x^{(i)})$$   

or, more commonly

$$\theta^*=\underset{\theta}{\operatorname{arg max}} \sum^{i=N}_{i=0} \log p_\theta (x^{(i)})$$   

{% include note.html 
    content="A good explanation of maximum likelihood in machine learning can be found in the [\"Deep Learning\" book](https://www.deeplearningbook.org/) by Goodfellow chapter 5.5"
%}

Now we need to define $$p_\theta(x)$$. The probability of drawing an image $$x$$ in this graphical model is  

$$p_\theta(x) = \displaystyle \int_{z}p_\theta(z)p_\theta(x\vert z)dz$$  

Let's think about what this means. To calculate the probability of a specific image of a cat $$p(x^{(i)})$$, we must go over all of $$z$$ and calculate the probability of generating $$x_i$$ given $$z$$.  
Of course, going over all possible values of $$z$$ is not something we can do, so we call $$p_\theta(x)$$  *intractable*, and this intractability is one of the central problems in bayesian statistics.  

There are several approaches to solving this intractability, one of them is *variational inference*, and the intuition behind this approach is the following. For a specific image $$x^{(i)}$$, there is a tiny area in $$z$$ with a high probability of generating $$x^{(i)}$$. For all other $$z$$, the probability is close to 0. For example, think about the image of the black cat from above; there is probably a tiny region in $$z$$ that represents black cats with the head on the left side and two open eyes. The probability of generating this cat black from the area representing white cats is ~0.


The key idea is instead of integrating all of $$z$$, we compute $$p_\theta(x^{(i)})$$ just by sampling from the tiny area in $$z$$, which is most likely to generate $$x^{(i)}$$. To find the area in $$z$$ most probable of generating $$x^{(i)}$$, we need the posterior $$p_\theta(z\vert x)$$. Unfortunately, the posterior is hidden from us, but! we can estimate it with a model $$q_\phi(z\vert x)$$ called the *probabilistic encoder*. 

This is starting to get the shape of a VAE:

![]({{ "/assets/vae/vae_p.png" | absolute_url }}){:height="40%" width="100%"}  

With this model, we will compute $$p_\theta(x^{(i)})$$ by first passing $$x^{(i)}$$ through the *probabilistic encoder* $$q_\phi(z\vert x)$$, and the output will be a small distribution over a tiny area in $$z$$. Then, we sample from that distribution and compute $$p_\theta(z\vert x)$$ on the samples. I'll get into these details later.

# Deriving the Objective - ELBO
Our juerney begins with the unknown posterior $$p_\theta(z\vert x)$$ and our estimation of it $$q_\phi(z\vert x)$$. We want our estimation to be as close to the true posterior as possible, and we can measure the distance between them using Kullback–Leibler divergence.

{% include note.html 
    content="Best intuition of KL-divergence is shown in [this](https://www.youtube.com/watch?v=ErfnhcEV1O8) video"
%}

<!-- 
&;&\text{definition of } D_{KL} \\
 &;&\text{bayes rule} 
 \text{log rules}
 -->

<div class="math-scroll">
$$\begin{eqnarray} 
D_{KL}[q_\phi(z\vert x)||p_\theta(z\vert x)] &=& \mathbb{E}_{z\sim q_\phi}[\log q_\phi(z\vert x)-\log p_\theta(z\vert x)]  \qquad      &;& (1) \\
D_{KL}[q_\phi(z\vert x)||p_\theta(z\vert x)] &=& \mathbb{E}_{z\sim q_\phi}[\log q_\phi(z\vert x)-\log {p_\theta(x\vert z)p_\theta(z)\over \log p_\theta(x)}]  \\
D_{KL}[q_\phi(z\vert x)||p_\theta(z\vert x)] &=& \mathbb{E}_{z\sim q_\phi}[\log q_\phi(z\vert x)-\log p_\theta(x\vert z)-\log p_\theta(z)-\log p_\theta(x)]   &;& \\
D_{KL}[q_\phi(z\vert x)||p_\theta(z\vert x)] &=& \mathbb{E}_{z\sim q_\phi}[\log q_\phi(z\vert x)-\log p_\theta(x\vert z)-\log p_\theta(z)]-\log p_\theta(x)   &;&\text{log rules} \\
\end{eqnarray}$$
</div>

1. Definition of KL-divergence.




<script src="https://utteranc.es/client.js"
        repo="yonigottesman/yonigottesman.github.io"
        issue-term="pathname"
        label="comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>



