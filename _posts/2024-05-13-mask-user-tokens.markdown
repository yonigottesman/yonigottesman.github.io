---
layout: post
title:  "Mask Your User Tokens"
excerpt: "While finetuning your llm on conversational datasets you should ignore the user tokens during backpropagation!"
date:   2024-05-13 00:00:00 +0000
categories: []
hide: false
---


* introduction ... fintuneing conversational dataset.
* lots of "no-code" finetune 
* tons of examples and hugging face defaults 
* reminder - on what is loss computed in decoder models
* does it make sense to learn to generate user text?
* how to ignore user tokens with -100
* results 
* conclusion










