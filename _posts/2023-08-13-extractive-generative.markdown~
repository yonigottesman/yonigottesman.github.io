---
layout: post
title:  "Extractive Question Answering with Generative Models"
excerpt: "some intro "
date:   2023-08-10 00:00:00 +0000
categories: []
hide: false
---
Generative models, such as ChatGPT and Llama2, excel in question-answering tasks given a specific context. For instance, consider the following prompt:

~~~
Given the following CT report, answer the question.
## REPORT
CT scan of the chest revealed the presence of multiple pulmonary nodules in the 
upper and middle lobes of both lungs. Nodules are of varying sizes,
with the largest measuring approximately 4 mm. 
Further evaluation and follow-up recommended to assess for any potential changes over
time.
## QUESTION
what is the location of the lung nodules
~~~

ChatGPT response is:
~~~
The CT report states that the pulmonary nodules are 
located in the upper and middle lobes of both lungs.
~~~
Impressive! However, the practical application of this output isn't always straightforward. For instance, when developing a medical application, physicians typically prefer direct references from the original text over a generative answer which might be inferred or "hallucinated".

In such scenarios, what physicians are often looking for is known as "Extractive QA". This method involves pinpointing an answer directly from the provided context and marking the `start` and `end` token locations within the text. In my example, it's not possible to pinpoint the response location because the response string isn't found verbatim in the context.

Typically, "Extractive QA" is achieved using encoder models like BERT. The output for each token indicates either the start or end of the answer, enabling us to pinpoint the exact location of the answer within the text.

![]({{ "/assets/extractive_generative/bertqa.png" | absolute_url }}){:height="50%" width="50%"}  

Inherently, outputting a `start`/`end` location of the answer is not possible with generative decoder models. These models are trained to predict the next word based on the preceding words. It's a bit of a puzzle; we have these sophisticated models like llama2, trained to deeply understand text, yet we can't use them for what might seem like a simpler task: pointing to an answer within a given context.

A workaround some might consider is tweaking the prompt to *encourage* the model into providing an answer directly from the context. For instance, adding instructions such as "answer the question without rephrasing".

While such strategies sometimes prove effective, I'd like to introduce a more robust solution: diving into the model's generation process. The goal here is to guide/constrain the model to output only a sequence of words present in the original context. 

## Extractive Genreation
Huggingface has a [convenient interface](https://huggingface.co/docs/transformers/main/main_classes/text_generation) called `generate` for text generation using autoregressive models. Users can set parameters like the maximum length of the generated text, specify end-of-sentence (eos) tokens, tweak beam search parameters, and much more.
Let's take a look at how we can use the generate interface with our earlier mentioned CT report:

~~~python
from transformers import GPT2Tokenizer, GPT2LMHeadModel

# Initialize the model and tokenizer
model = GPT2LMHeadModel.from_pretrained("gpt2-medium")
tokenizer = GPT2Tokenizer.from_pretrained("gpt2-medium")

# Our CT report context
context = """
CT scan of the chest revealed the presence of multiple pulmonary nodules in the upper and middle lobes of both lungs. 
The nodules vary in size, with the largest measuring approximately 4 mm. 
Further evaluation and follow-up are recommended to assess any potential changes over time.
"""

# Tokenize input context
input_ids = tokenizer.encode(context, return_tensors='pt')

# Generate text with the `generate` method
output = model.generate(input_ids, max_length=200, num_beams=5, eos_token_id=tokenizer.eos_token_id)

# Decode the generated text
decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)

print(decoded_output)
~~~


<script src="https://utteranc.es/client.js"
        repo="yonigottesman/yonigottesman.github.io"
        issue-term="pathname"
        label="comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>



