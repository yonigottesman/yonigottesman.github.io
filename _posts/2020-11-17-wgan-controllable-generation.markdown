---
layout: post
title:  "Controllable Generation with Fixed GANs"
excerpt: "Controll the facial features of a fake image generated by a fixed WGAN only by moving around the latent space in the right direction."
date:   2020-11-17 00:00:00 +0200
categories: [pytorch,gan,wgan]
permalink: /2020/11/17/wgan-controllable-generation.html/
hide: false
---

<!-- Mathjax Support -->
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>



Conditional GAN architectures such as cGAN learn to generate fake images with a specific condition. For example the generator can be forced to generate images of people smiling, with or without glasses or with a specific hair color. But in order to achieve this, the GAN must be trained alongside with the conditional information. What if you are given a fixed trained GAN that was not trained with additional condition information, is it still possible to force the generator to generate images with specific features? according to this paper: [Interpreting the Latent Space of GANs for Semantic Face Editing](https://arxiv.org/abs/1907.10786), YES you can! 
The paper explores how different features are encoded in the latent space and show how by moving around this space you can control the features of a generated fake image.  
In this post I will show how to turn a fixed unconstrained GAN to a controllable GAN just by moving around the latent space in the right direction.

All the code can be found on my [github](https://github.com/yonigottesman/controllable_generation_gan) and the fixed trained GAN used is  wGAN I implemented  [here](https://github.com/yonigottesman/controllable_generation_gan/blob/main/wgan-gp-celeba.ipynb).


{% include note.html 
    content="I learned these ideas in the deeplearning.ai GANs [specialization](https://www.coursera.org/specializations/generative-adversarial-networks-gans?utm_source=deeplearningai&utm_medium=institutions&utm_campaign=DLWebGANsMain)." %}

Latent Space Interpolation
==
The generator of a trained GAN is a function $$ g:Z \to X $$ where $$ Z \subseteq  \mathbb{R}^d $$ is the $$d$$-dimensional latent space from which a noise vector is drawn and $$X$$ is the image space. Whats cool about this mapping is that small changes in a random vector $$z \in Z$$ correspond to small changes in the generated image. To visualize this coolness I can draw two vectors $$z1,z2 \in Z$$ and sample vectors on the linear interpolant between them:

![interpolation_latent]({{ "/assets/controll_gan/interpolation_latent.png" | absolute_url }})

Then pass each vector through the generator and display the output:

```python
z1 = torch.randn(1, 100, 1, 1, device=device)
z2 = torch.randn(1, 100, 1, 1, device=device)

fakes = []
for i in torch.linspace(0,1,10):
    with torch.no_grad():
        fake = generator(z1*i+z2*(1-i))
    fakes.append(fake)

mymshow(utils.make_grid(torch.cat(fakes,0),nrow=10,normalize=True),fig_size=[100,100])
```

![interpolation]({{ "/assets/controll_gan/interpolation.png" | absolute_url }})

$$z1$$ and $$z2$$ are the left and rightmost images and you can see a smooth transition between them. 



Controllable Generation
== 
Just like with image interpolation, controllable generation also takes advantage of the fact that small changes in the latent space correspond to small changes in the generator output, except instead of moving on an interpolation between two vectors we move in a direction that only changes a single feature of the image. 
For example, if the output of the generator for a vector $$z1$$ is a man without glasses, and we want to generate a man with glasses, we can move in a direction $$n$$ that (nearly) doesn't change anything in the image except adding the man glasses: $$ z1_{new}=z1+\alpha n$$ ($$\alpha$$=step size).  
The questions are how do we know this direction even exists? and if it does how do we find it?

Latent Space Separation
==
The paper makes the assumption that for any binary feature (*e.g.*, glasses\no-glasses, smiling\not-smiling, male\female) there  exists  a  hyperplane  in  the latent space serving as the separation boundary. For example if $$Z \subseteq \mathbb{R}^2$$ then there exists a line that separates points that generate images with or without a smile.

![separation]({{ "/assets/controll_gan/separation.png" | absolute_url }})

Starting with a vector $$z$$ and moving in the latent space, the feature we are trying to change will remain the same as long as we are on the same side of the hyperplane. When the boundary is crossed, the feature turns into the opposite. To flip a feature in the fake image, the random $$z$$ must be edited to be closer and closer to the boundary until it crosses it:

![separation_edit]({{ "/assets/controll_gan/separation_edit.png" | absolute_url }})


Starting with $$z_{0}$$ that generates an image without a smile :-( and editing it with $$z=z+\alpha \vec n$$ where $$\vec n $$ is the normal vector of the hyperplane until the boundary is crossed and the generated image is smiling :-).  

To learn these boundries, tuples of <latent vector, feature score> where generated and used to train independent linear SVMs for each feature. They published the code [here](https://github.com/genforce/interfacegan/blob/master/train_boundary.py).


Classifier Gradients
==
Instead of learning separation boundaries and editing $$z$$ towards them, A different technique can be used. A pre-trained feature classifier can be applied on a fake image to calculate the probability of having a specific feature. Then $$z$$ can be edited according to the gradients of the probability with respect to $$z$$ to improve the probability. This iterative process does **not** train the GAN or the classifier, its "training" the noise vector $$z$$ to output fake images with/without a feature.

![classifier_grad]({{ "/assets/controll_gan/classifier_grad.png" | absolute_url }})
*deeplearning.ai*

Classifier
==
To train the feature classifier, I use the celeba dataset with facial attributes:
```python
mean,std=[0.5, 0.5, 0.5], [0.5, 0.5, 0.5] # Same as when training the GAN.
compose = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.Resize(64),
    transforms.CenterCrop(64),
    transforms.ToTensor(),
    transforms.Normalize(mean=mean, std=std)
])

train_dataset = torchvision.datasets.CelebA('celeba',split='train',target_type='attr', transform=compose, download=True)
valid_dataset = torchvision.datasets.CelebA('celeba',split='valid',target_type='attr', transform=compose, download=True)
```

Each image in the dataset has a hot encoded tensor of 40 features but I am only going to train to classify 4 of them:
```python
relevant_attributes = ['Black_Hair', 'Blond_Hair', 'Eyeglasses','Smiling']
```

Resnet18 model with a 4 output linear layer followed by sigmoid to calculate the probability of each feature. 
```python
class NNet(nn.Module):
    def __init__(self, num_attributes):
        super(NNet, self).__init__()
        # NOT pretrained because normalization parameters are different.
        self.classifier = torchvision.models.resnet18(pretrained=False) 
        self.classifier.fc = nn.Linear(self.classifier.fc.in_features, num_attributes)
    def forward(self, xb):
        yhat = self.classifier(xb).sigmoid() # NOT softmax because multilabel!
        return yhat
```
Standard training loop code is [here](https://github.com/yonigottesman/controllable_generation_gan/blob/main/classifier.ipynb).


Test an example image from the dev_set and classify feature as True if prob > 0.5:
```python
classifier.eval()
print(f'label = {valid_dataset[idx][1][relevant_indices]}')
with torch.no_grad():
    print(f'prediction = {classifier(valid_dataset[idx][0].unsqueeze(0).to(device)) > 0.5}')
mymshow(valid_dataset[idx][0],std,mean) 
```
![classifier_check]({{ "/assets/controll_gan/classifier_check.png" | absolute_url }}){:height="20%" width="20%"}  
```
# ['Black_Hair', 'Blond_Hair', 'Eyeglasses','Smiling']
label = tensor([0, 0, 1, 0])
prediction = tensor([[False, False,  True, False]], device='cuda:0')
```
Looking good!

Editing Latent Space
==
I now have all the building blocks I need: A fixed wgan trained without feature information and a classifer that can score features. I can now edit a random $$z$$ vector and change specific features in the fake image. Lets try adding a smile to a fake image.

Sample a random vector $$z$$ and allow gradient computation on it. We need gradients to do a gradient ascent towards improving the feature probablity the classifier gives the image.
```python
z = torch.randn(1, 100, 1, 1, device=device).requires_grad_()
```
The fake image generated from $$z$$ and the probability of it containing a smile:
```python
with torch.no_grad():
    fake = generator(z0)
smile_p = classifier(fake).squeeze(0)[SMILE_INDEX].item()
mymshow(fake.squeeze(0), std,mean,xlabel=f'p(smile)={smile_p:.2f}')
```
![first_smile]({{ "/assets/controll_gan/first_smile.png" | absolute_url }}){:height="40%" width="40%"}  
To get this image to smile we are about to edit $$z$$ in small steps, each step will make the image a bit more happy :-)

The iterative process is:
 1. Generate fake image from $$z$$. 
 2. Score fake image feature using classifier.
 3. Compute gradients of the score with respect to $$z$$.
 4. Perform gradient ascent on $$z$$ in the direction of improving the score.

This is done while the probability of a smile is under 0.995:

```python
lr=0.1
while smile_p < 0.995:
    classifier.zero_grad()
    fake = generator(z) # 1
    smile_p = classifier(fake).squeeze(0)[SMILE_INDEX] # 2
    smile_p.backward() # 3
    z.data = z + (z.grad*lr) # 4
```

In my [implementation]() I store the fake image in each step so I can later display how the fake image changes until its smiling:  

![smile generation]({{ "/assets/controll_gan/smile_generation.png" | absolute_url }}){:height="100%" width="100%"}


Conclusion
==
In this post I showed how to take a fixed WGAN trained on celeba and with a feature classier manage to walk around the latent space in the direction of adding or removing a feature.

<!-- Feature Entanglement -->
<!-- == -->


<!-- Real Image Manipulation -->
<!-- == -->
